# Search-R1 Agentic RL å­¦ä¹ è®¡åˆ’

> åŸºäºå¼ºåŒ–å­¦ä¹ çš„æ¨ç†-æœç´¢äº¤ç»‡LLMè®­ç»ƒé¡¹ç›®

---

## é¡¹ç›®æ¦‚è¿°

**Search-R1** æ˜¯ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºè®­ç»ƒå…·å¤‡æ¨ç†å’Œæœç´¢å·¥å…·è°ƒç”¨èƒ½åŠ›çš„LLM Agentã€‚

- **æ ¸å¿ƒç›®æ ‡**ï¼šè®©åŸºç¡€æ¨¡å‹é€šè¿‡RLå­¦ä¼šåœ¨æ¨ç†è¿‡ç¨‹ä¸­ä¸»åŠ¨è°ƒç”¨æœç´¢å·¥å…·
- **æŠ€æœ¯è·¯çº¿**ï¼šPPO/GRPO + Tool Calling + Rule-based Reward
- **å¯¹æ ‡æ–¹æ¡ˆ**ï¼šOpenAI DeepResearch
- **å¼€æºåè®®**ï¼šApache 2.0

---

## å­¦ä¹ ç›®æ ‡

1. æŒæ¡Agentic RLçš„æ ¸å¿ƒè®­ç»ƒèŒƒå¼
2. ç†è§£PPO/GRPOç®—æ³•çš„å·¥ç¨‹å®ç°
3. å­¦ä¼šTool Callingæœºåˆ¶çš„è®¾è®¡ä¸å®ç°
4. ç§¯ç´¯åˆ†å¸ƒå¼è®­ç»ƒå’Œæ¨ç†ä¼˜åŒ–çš„ç»éªŒ
5. èƒ½å¤Ÿå‘é¢è¯•å®˜æ¸…æ™°é˜è¿°æŠ€æœ¯ç»†èŠ‚

---

## å­¦ä¹ è·¯çº¿å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Phase 1: é¡¹ç›®æ¶æ„ç†è§£ (1-2å¤©)                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Phase 2: RLç®—æ³•å®ç° (2-3å¤©)                                     â”‚
â”‚     â”œâ”€â”€ PPOæ ¸å¿ƒæµç¨‹                                              â”‚
â”‚     â”œâ”€â”€ GRPOç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–                                        â”‚
â”‚     â””â”€â”€ GAEä¼˜åŠ¿ä¼°è®¡                                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Phase 3: Tool Callingæœºåˆ¶ (2å¤©)                                â”‚
â”‚     â”œâ”€â”€ æœç´¢APIè°ƒç”¨æµç¨‹                                          â”‚
â”‚     â”œâ”€â”€ å¤šè½®äº¤äº’å®ç°                                             â”‚
â”‚     â””â”€â”€ æ£€ç´¢å™¨é€‰å‹                                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Phase 4: è®­ç»ƒTrickä¸ä¼˜åŒ– (2-3å¤©)                               â”‚
â”‚     â”œâ”€â”€ FSDPåˆ†å¸ƒå¼è®­ç»ƒ                                          â”‚
â”‚     â”œâ”€â”€ vLLMæ¨ç†åŠ é€Ÿ                                            â”‚
â”‚     â””â”€â”€ è®­ç»ƒç¨³å®šæ€§è°ƒä¼˜                                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Phase 5: æ•°æ®æµä¸åè®® (1-2å¤©)                                  â”‚
â”‚     â”œâ”€â”€ verlæ•°æ®æ ¼å¼                                            â”‚
â”‚     â””â”€â”€ å¥–åŠ±å‡½æ•°è®¾è®¡                                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Phase 6: å®éªŒå¤ç°ä¸è°ƒå‚ (æŒç»­)                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Phase 1: é¡¹ç›®æ•´ä½“æ¶æ„ç†è§£

### 1.1 æ ¸å¿ƒæ¦‚å¿µ

| æ¦‚å¿µ | è§£é‡Š |
|------|------|
| **Reasoning-Search Interleaved** | æ¨ç†ä¸æœç´¢äº¤æ›¿è¿›è¡Œï¼ŒAgentè‡ªä¸»å†³å®šä½•æ—¶æœç´¢ |
| **Tool Calling Agent** | èƒ½å¤Ÿè°ƒç”¨å¤–éƒ¨å·¥å…·ï¼ˆæœç´¢APIï¼‰çš„æ™ºèƒ½ä½“ |
| **Rule-based RL** | åŸºäºè§„åˆ™çš„å¥–åŠ±ä¿¡å·ï¼Œæ— éœ€å¥–åŠ±æ¨¡å‹ |
| **Outcome Supervision** | ä»…å¯¹æœ€ç»ˆç»“æœè¿›è¡Œç›‘ç£ï¼Œä¸ç›‘ç£ä¸­é—´è¿‡ç¨‹ |

### 1.2 æ•´ä½“æ•°æ®æµ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   DataLoader  â”‚ -> â”‚  RayPPO      â”‚ -> â”‚   Rollout    â”‚
â”‚  (Parquet)   â”‚    â”‚   Trainer    â”‚    â”‚  (vLLM)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                               â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    v
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚   LLMç”Ÿæˆ    â”‚ -> â”‚  Search API  â”‚ <- â”‚  å†³å®šæ˜¯å¦æœç´¢ â”‚
           â”‚  åˆ°ç‰¹æ®Štoken â”‚    â”‚   è°ƒç”¨       â”‚    â”‚  (<search>)  â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                       â”‚
                                       v
                               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                               â”‚  Rewardè¯„ä¼°   â”‚
                               â”‚  (EMåŒ¹é…)     â”‚
                               â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                       â”‚
                                       v
                               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                               â”‚  PPO/GRPOæ›´æ–° â”‚
                               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.3 å…³é”®æ–‡ä»¶é€Ÿè§ˆ

| è·¯å¾„ | åŠŸèƒ½ |
|------|------|
| `train_ppo.sh` | PPOè®­ç»ƒå…¥å£è„šæœ¬ |
| `train_grpo.sh` | GRPOè®­ç»ƒå…¥å£è„šæœ¬ |
| `verl/trainer/main_ppo.py` | PPOè®­ç»ƒä¸»é€»è¾‘ |
| `verl/trainer/ppo/core_algos.py` | PPO/GRPOæ ¸å¿ƒç®—æ³• |
| `verl/workers/rollout/` | Rolloutç”Ÿæˆæ¨¡å— |
| `search_r1/search/retrieval.py` | æ£€ç´¢å™¨å®ç° |
| `infer.py` | æ¨ç†æµ‹è¯•è„šæœ¬ |

### 1.4 å­¦ä¹ ä»»åŠ¡

- [x] é˜…è¯»README.mdï¼Œç†è§£é¡¹ç›®å®šä½
- [x] æµè§ˆtrain_ppo.shï¼Œç†Ÿæ‚‰è®­ç»ƒé…ç½®
- [x] æŸ¥çœ‹verlç›®å½•ç»“æ„ï¼Œäº†è§£veRLæ¡†æ¶
- [x] ç»˜åˆ¶æ•´ä½“æ¶æ„å›¾

---

### ğŸ“š Phase 1 å­¦ä¹ ç¬”è®°ï¼ˆå·²æŒæ¡ï¼‰

#### Q1: é¡¹ç›®æ ¸å¿ƒç›®æ ‡
- **Search-R1** æ ¸å¿ƒç›®æ ‡ï¼šé€šè¿‡å¼ºåŒ–å­¦ä¹ è®©æ¨¡å‹å­¦ä¼šåœ¨æ¨ç†è¿‡ç¨‹ä¸­ä¸»åŠ¨è°ƒç”¨æœç´¢å·¥å…·
- æŠ€æœ¯è·¯çº¿ï¼šPPO/GRPO + Tool Calling + Rule-based Reward

#### Q2: Rule-based Reward å®šä¹‰
- æ•°æ®é›†ä¸­åŒ…å« `ground_truth` æ ‡å‡†ç­”æ¡ˆ
- é€šè¿‡ **Exact Match (EM)** åŒ¹é…è®¡ç®—å¥–åŠ±ï¼ˆ0æˆ–1ï¼‰
- å¥–åŠ±åªåœ¨**æœ€åä¸€ä¸ª token ä½ç½®**ç»™å‡ºï¼ˆOutcome Supervisionï¼‰
- å‚è€ƒå®ç°ï¼š`verl/utils/reward_score/qa_em.py`

#### Q3: å·¥å…·è°ƒç”¨æœºåˆ¶
- å®šä¹‰ **special token**ï¼ˆå¦‚ `<search>`ï¼‰
- æ£€æµ‹åˆ°ç‰¹æ®Š token â†’ æš‚åœç”Ÿæˆ â†’ æå–æŸ¥è¯¢è¯ â†’ è°ƒç”¨æœç´¢API â†’ æ‹¼æ¥ç»“æœåˆ°ä¸Šä¸‹æ–‡ â†’ ç»§ç»­ç”Ÿæˆ
- è¿™å°±æ˜¯ **Reasoning-Search Interleaved** æ ¸å¿ƒæœºåˆ¶

#### Q4: æ£€ç´¢å™¨åŒºåˆ«
- **BM25ï¼ˆç¨€ç–æ£€ç´¢ï¼‰**ï¼šåŸºäºè¯é¢‘ç»Ÿè®¡çš„ä¼ ç»Ÿç®—æ³•ï¼Œä¸éœ€è¦è®­ç»ƒï¼Œæ— æ³•ç†è§£è¯­ä¹‰
- **E5/BGEï¼ˆç¨ å¯†æ£€ç´¢ï¼‰**ï¼šåŸºäº Encoder-only Transformer çš„ç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œéœ€è¦é¢„è®­ç»ƒï¼Œé€šè¿‡å‘é‡è¯­ä¹‰åŒ¹é…
- æœ¬è´¨åŒºåˆ«ï¼šå­—ç¬¦ä¸²åŒ¹é… vs è¯­ä¹‰å‘é‡åŒ¹é…

#### Q5: PPO vs GRPO ä¼˜åŠ¿ä¼°è®¡åŒºåˆ«
- **PPO**ï¼šéœ€è¦ Critic æ¨¡å‹ï¼Œä½¿ç”¨ GAE ä¼°è®¡ä¼˜åŠ¿
- **GRPO**ï¼šä¸éœ€è¦ Criticï¼Œå¯¹åŒä¸€é—®é¢˜é‡‡æ ·å¤šä¸ªç­”æ¡ˆï¼Œè®¡ç®—ç»„å†…ç›¸å¯¹æ’åä½œä¸ºä¼˜åŠ¿
- å…¬å¼ï¼š$A_i = \frac{score_i - mean}{std + \epsilon}$

#### Q6: æ¨¡å‹è§„æ¨¡ä¸å¤šè½®æœç´¢
- **3B æ¨¡å‹**ï¼šåªèƒ½å•è½®æœç´¢ï¼ˆæ¨¡å‹å®¹é‡ä¸è¶³+è®­ç»ƒä¸ç¨³å®š+ä¸Šä¸‹æ–‡è†¨èƒ€ï¼‰
- **7B æ¨¡å‹**ï¼šå¯ä»¥å¤šè½®æœç´¢ï¼ˆScaling Law å†³å®šèƒ½åŠ›ä¸Šé™ï¼‰
- å¤šè½®æœç´¢ç“¶é¢ˆï¼šæœç´¢è¿”å›å†…å®¹é•¿ï¼Œä¸Šä¸‹æ–‡çª—å£è¿…é€Ÿè†¨èƒ€

#### Q7: Search-R1 vs RAG
- **ä¼ ç»Ÿ RAG**ï¼šç”¨æˆ· query è¿›æ¥åè¢«åŠ¨æ£€ç´¢
- **Search-R1**ï¼šæ¨¡å‹è‡ªä¸»å†³å®šã€Œä½•æ—¶è¯¥æœã€ã€Œæœä»€ä¹ˆã€ã€Œå¦‚ä½•æ•´åˆã€
- æ ¸å¿ƒåŒºåˆ«ï¼šæ¨¡å‹å­¦åˆ°äº†**ä¸»åŠ¨åˆ¤æ–­**èƒ½åŠ›

---

## Phase 2: å¼ºåŒ–å­¦ä¹ ç®—æ³•å®ç°

### 2.1 PPOç®—æ³•è¯¦è§£

#### æ ¸å¿ƒå…¬å¼

**1. GAEä¼˜åŠ¿ä¼°è®¡ (Generalized Advantage Estimation)**
```
Î´_t = r_t + Î³ * V(s_{t+1}) - V(s_t)
A_t = Î´_t + Î³ * Î» * Î´_{t+1} + (Î³ * Î»)^2 * Î´_{t+2} + ...
```

**2. PPO Clipæ›´æ–°**
```
L = min( r(Î¸) * A_t, clip(r(Î¸), 1-Îµ, 1+Îµ) * A_t )
```
å…¶ä¸­ r(Î¸) = Ï€_Î¸(a|s) / Ï€_Î¸_old(a|s)

**3. KLæ•£åº¦æƒ©ç½š**
```
KL = log(Ï€_new) - log(Ï€_old)
Loss = L - Î² * KL
```

#### ä»£ç å®ç°ä½ç½®

```python
# verl/trainer/ppo/core_algos.py

def compute_gae_advantage_return(token_level_rewards, values, eos_mask, gamma, lam):
    """GAEä¼˜åŠ¿è®¡ç®—"""
    # Î´_t = r_t + Î³ * V_{t+1} - V_t
    # A_t = Î£ (Î³Î»)^l * Î´_l

def compute_policy_loss(old_log_prob, log_prob, advantages, eos_mask, cliprange):
    """PPOç­–ç•¥æŸå¤±"""
    # L = min(r * A, clip(r, 1-Îµ, 1+Îµ) * A)

def compute_value_loss(vpreds, returns, values, eos_mask, cliprange_value):
    """å€¼å‡½æ•°æŸå¤±"""
    # L = 0.5 * max((v - target)^2, (clip(v) - target)^2)
```

### 2.2 GRPOç®—æ³•è¯¦è§£

#### æ ¸å¿ƒæ€æƒ³

GRPO (Group Relative Policy Optimization) ä¸éœ€è¦å€¼å‡½æ•°ï¼Œé€šè¿‡ç»„å†…ç›¸å¯¹æ’åè®¡ç®—ä¼˜åŠ¿ï¼š

```python
# å¯¹æ¯ä¸ªé—®é¢˜ç”ŸæˆGä¸ªç­”æ¡ˆ
scores = [score_1, score_2, ..., score_G]

# è®¡ç®—ç»„å†…å‡å€¼å’Œæ ‡å‡†å·®
mean = Î£scores / G
std = sqrt(Î£(scores - mean)^2 / G)

# ç›¸å¯¹ä¼˜åŠ¿
A_i = (score_i - mean) / (std + Îµ)
```

#### ä»£ç å®ç°

```python
def compute_grpo_outcome_advantage(token_level_rewards, eos_mask, index, epsilon=1e-6):
    """GRPOä¼˜åŠ¿è®¡ç®—"""
    # 1. æŒ‰é—®é¢˜åˆ†ç»„
    # 2. è®¡ç®—ç»„å†…å‡å€¼å’Œæ ‡å‡†å·®
    # 3. æ ‡å‡†åŒ–å¾—åˆ†ä½œä¸ºä¼˜åŠ¿
```

### 2.3 PPO vs GRPO å¯¹æ¯”

| ç»´åº¦ | PPO | GRPO |
|------|-----|------|
| **å€¼å‡½æ•°** | éœ€è¦Criticæ¨¡å‹ | ä¸éœ€è¦ |
| **æ ·æœ¬æ•ˆç‡** | è¾ƒä½ï¼ˆæ¯ä¸ªé—®é¢˜1ä¸ªç­”æ¡ˆï¼‰ | è¾ƒé«˜ï¼ˆæ¯ä¸ªé—®é¢˜Nä¸ªç­”æ¡ˆï¼‰ |
| **è®­ç»ƒç¨³å®šæ€§** | è¾ƒå¥½ï¼ˆClipæœºåˆ¶ï¼‰ | ä¸€èˆ¬ |
| **è®¡ç®—å¼€é”€** | å¤§ï¼ˆåŒæ¨¡å‹+å€¼å‡½æ•°ï¼‰ | å°ï¼ˆå•æ¨¡å‹ï¼‰ |
| **é€‚ç”¨åœºæ™¯** | å¤æ‚ä»»åŠ¡ã€é•¿æœŸè§„åˆ’ | ç®€å•ä»»åŠ¡ã€å¿«é€ŸéªŒè¯ |

### 2.4 å…³é”®å‚æ•°å«ä¹‰

```yaml
# PPOé…ç½®ç¤ºä¾‹
algorithm:
  adv_estimator: gae  # æˆ– grpo
  gamma: 1.0          # æŠ˜æ‰£å› å­
  lam: 0.95           # GAE lambda

actor_rollout_ref.actor:
  ppo_micro_batch_size: 64    # æ¯ä¸ªmicro batchçš„å¤§å°
  ppo_mini_batch_size: 256    # æ¯ä¸ªmini batchçš„å¤§å°
  cliprange: 0.2              # PPO clipèŒƒå›´
  kl_loss_coef: 0.001         # KLç³»æ•°
```

### 2.5 å­¦ä¹ ä»»åŠ¡

- [x] ç²¾è¯» `core_algos.py`ï¼Œæ‰‹å†™æ¯ä¸ªæ ¸å¿ƒå‡½æ•°
- [x] å¯¹æ¯”PPOå’ŒGRPOçš„ä¼˜ç¼ºç‚¹
- [x] ç†è§£GAEä¸­Î³å’ŒÎ»çš„ä½œç”¨
- [x] æ¨å¯¼PPO Gradientå…¬å¼

---

### ğŸ“š Phase 2 è¡¥å……å­¦ä¹ ç¬”è®°ï¼ˆå·²æŒæ¡ - 2025-02-13ï¼‰

#### Q8: GAEä¸­Î³å’ŒÎ»çš„åŒºåˆ«
- **Î³ (gamma)**: æŠ˜æ‰£å› å­ï¼Œä½œç”¨äº**ä»·å€¼ä¼°è®¡æœ¬èº«**ï¼Œæ§åˆ¶æœªæ¥å¥–åŠ±çš„ä»·å€¼æŠ˜æ‰£
- **Î» (lambda)**: GAEå‚æ•°ï¼Œä½œç”¨äº**TDè¯¯å·®åºåˆ—çš„æƒé‡**ï¼Œæ§åˆ¶æ–¹å·®-åå·®æƒè¡¡
- ä¸¤è€…ä¸èƒ½åˆå¹¶ï¼šÎ³æ§åˆ¶"ä»·å€¼ä¼°è®¡çš„æ—¶é—´å°ºåº¦"ï¼ŒÎ»æ§åˆ¶"ä¼˜åŠ¿ä¼°è®¡çš„ä¼°è®¡æ–¹å¼"

#### Q9: ä¸ºä»€ä¹ˆè¦ç´¯åŠ å¤šä¸ªÎ´ï¼Ÿ
- Î´_t = r_t + Î³*V(s_{t+1}) - V(s_t) åªæ˜¯å•æ­¥ä»·å€¼å˜åŒ–
- å•æ­¥Î´ä¿¡æ¯ä¸å…¨ï¼Œéœ€è¦èšåˆå¤šæ­¥ä¿¡æ¯æ¥ä¼°è®¡çœŸå®çš„ä¼˜åŠ¿
- GAEæœ¬è´¨æ˜¯åå·®-æ–¹å·®æƒè¡¡ï¼šç”¨å¤šæ­¥TDè¯¯å·®åŠ æƒå¹³å‡

#### Q10: PPO vs DQN
- **DQN**: å€¼å‡½æ•°æ–¹æ³•ï¼Œç›´æ¥å­¦ä¹ Q(s,a)ï¼Œç”¨å•æ­¥TDæ›´æ–°
- **PPO**: ç­–ç•¥æ¢¯åº¦æ–¹æ³•ï¼Œéœ€è¦æ˜¾å¼è®¡ç®—ä¼˜åŠ¿A(s,a)æ¥æ›´æ–°ç­–ç•¥

#### Q11: PPO Clipæœºåˆ¶
- L = min(r(Î¸)*A, clip(r, 1-Îµ, 1+Îµ)*A)
- **L**æ˜¯æŸå¤±å‡½æ•°/ç›®æ ‡å‡½æ•°
- **r(Î¸)** = Ï€_Î¸(a|s) / Ï€_Î¸_old(a|s)ï¼Œæ¦‚ç‡æ¯”å€¼
- Clipæ€æƒ³å·²å¹¿æ³›åº”ç”¨äºå…¶ä»–ç®—æ³•ï¼Œä¸å±€é™äºPPO

#### Q12: KLæ•£åº¦
- KL(P||Q) = Î£ P(x) * log(P(x)/Q(x))
- PPOä¸­ç”¨KLé™åˆ¶æ–°ç­–ç•¥ä¸è¦åç¦»æ—§ç­–ç•¥å¤ªè¿œ
- æœ‰ä¸¤ç§å½¢å¼ï¼šKLæƒ©ç½šï¼ˆ-Î²*KLï¼‰å’ŒKLçº¦æŸï¼ˆTRPOï¼‰

#### Q13: GRPOæ ¸å¿ƒæ€æƒ³
- ä¸éœ€è¦å€¼å‡½æ•°(Critic)
- å¯¹åŒä¸€é—®é¢˜é‡‡æ ·Gä¸ªç­”æ¡ˆï¼Œè®¡ç®—ç»„å†…ç›¸å¯¹æ’åä½œä¸ºä¼˜åŠ¿
- A_i = (score_i - mean) / (std + Îµ)
- ä¼˜ç‚¹ï¼šè®¡ç®—å¼€é”€å°ã€æ ·æœ¬æ•ˆç‡é«˜
- å±€é™ï¼šä¸é€‚åˆå¤æ‚æ¨ç†ã€ç¨€ç–å¥–åŠ±

#### Q14: Search-R1ç”¨çš„æ˜¯å“ªç§PPOç¨³å®šåŒ–æ–¹æ³•ï¼Ÿ
- **å½“å‰å®ç°**ï¼šä½¿ç”¨ KLæƒ©ç½šï¼ˆkl_coef=0.001ï¼‰
- **é…ç½®ä½ç½®**ï¼štrain_ppo.sh ä¸­çš„ `algorithm.kl_ctrl.kl_coef=0.001`
- **å¦‚ä½•æ”¹ç”¨Clip**ï¼šåœ¨é…ç½®ä¸­æ·»åŠ  `actor_rollout_ref.actor.cliprange=0.2`
- **å¯¹æ¯”å®éªŒå»ºè®®**ï¼šå¯ä»¥å°è¯•åŒæ—¶ä½¿ç”¨KLæƒ©ç½š+Clipï¼Œæˆ–è€…åªç”¨Clipå¯¹æ¯”æ•ˆæœ

---

## Phase 3: Tool Calling æœºåˆ¶

### 3.1 Agentå†³ç­–æµç¨‹

```
User: "Who is the CEO of Tesla?"

1. LLMç”Ÿæˆ: "Let me think... I need to search for this information <search>"
2. æ£€æµ‹åˆ°<search> token
3. æš‚åœç”Ÿæˆï¼Œæå–æŸ¥è¯¢è¯: "Tesla CEO"
4. è°ƒç”¨æœç´¢API: POST /retrieve?query="Tesla CEO"
5. è·å¾—æ£€ç´¢ç»“æœ:
   {
     "docs": [
       {"title": "Elon Musk", "contents": "Elon Musk is the CEO..."},
       {"title": "Tesla Leadership", "contents": "Tesla was founded..."}
     ]
   }
6. å°†ç»“æœæ’å…¥å¯¹è¯:
   [{"role": "user", "content": "..."}, 
    {"role": "assistant", "content": "Let me search..."},
    {"role": "tool", "content": "[docs...]"}]
7. LLMç»§ç»­ç”Ÿæˆ: "Based on the search results, Elon Musk is the CEO..."
8. æœ€ç»ˆç­”æ¡ˆéªŒè¯
```

### 3.2 å¤šè½®æœç´¢é…ç½®

```yaml
# æœ€å¤§2è½®æœç´¢
max_turns: 2

# æ¯æ¬¡æ£€ç´¢top-3ç»“æœ
retriever:
  topk: 3
  url: "http://127.0.0.1:8000/retrieve"
```

### 3.3 æ£€ç´¢å™¨ç±»å‹

| ç±»å‹ | å®ç° | ä¼˜ç‚¹ | ç¼ºç‚¹ |
|------|------|------|------|
| **BM25** | Pyserini | è½»é‡ã€æ— éœ€è®­ç»ƒ | è¯­ä¹‰ç†è§£å¼± |
| **Dense (E5)** | Faiss + Encoder | è¯­ä¹‰åŒ¹é…å¼º | éœ€è¦ç´¢å¼•æ„å»º |
| **Dense (BGE)** | Faiss + Encoder | ä¸­æ–‡æ•ˆæœå¥½ | èµ„æºæ¶ˆè€—å¤§ |
| **åœ¨çº¿æœç´¢** | Google/Bing API | å®æ—¶æ•°æ® | æˆæœ¬é«˜ã€æœ‰å»¶è¿Ÿ |

### 3.4 æ£€ç´¢å™¨ä»£ç å®ç°

```python
# search_r1/search/retrieval.py

class DenseRetriever:
    def __init__(self, config):
        self.index = faiss.read_index(config.index_path)
        self.encoder = Encoder(model_path=config.retrieval_model_path)
    
    def _search(self, query, num=3):
        # 1. queryç¼–ç 
        query_emb = self.encoder.encode(query)
        # 2. Faissæ£€ç´¢
        scores, idxs = self.index.search(query_emb, k=num)
        # 3. è¿”å›æ–‡æ¡£
        results = load_docs(self.corpus, idxs)
        return results
```

### 3.5 å­¦ä¹ ä»»åŠ¡

- [x] å¤ç°ä¸€ä¸ªç®€å•çš„BM25æ£€ç´¢å™¨
- [x] ç†è§£E5/BGEçš„ç¼–ç å’Œç´¢å¼•åŸç†
- [x] è®¾è®¡ä¸€ä¸ªAgent Tool Callingçš„æç¤ºæ¨¡æ¿
- [x] æµ‹è¯•å¤šè½®æœç´¢çš„æ•ˆæœ

---

### ğŸ“š Phase 3 è¡¥å……å­¦ä¹ ç¬”è®°ï¼ˆå·²æŒæ¡ - 2025-02-13ï¼‰

#### Q14: EMåŒ¹é… (Exact Match)
- æ£€æŸ¥æ¨¡å‹ç”Ÿæˆçš„ç­”æ¡ˆæ˜¯å¦ä¸æ ‡å‡†ç­”æ¡ˆ**å®Œå…¨ä¸€è‡´**
- é€šå¸¸ä¼šåšé¢„å¤„ç†ï¼šè½¬å°å†™ã€å»é™¤æ ‡ç‚¹ã€å»é™¤ç©ºæ ¼

#### Q15: Tool-Callingå¦‚ä½•æˆªæ–­è¾“å‡º
- æ£€æµ‹ç‰¹æ®Štokenï¼ˆå¦‚`<search>`ï¼‰
- æˆªæ–­ï¼šåªä¿ç•™ç‰¹æ®Štokenä¹‹å‰çš„æ–‡æœ¬
- æå–ï¼šä»æˆªæ–­æ–‡æœ¬ä¸­æå–æŸ¥è¯¢è¯
- æ³¨å…¥ï¼šå°†æœç´¢ç»“æœä½œä¸ºtoolæ¶ˆæ¯æ’å…¥ä¸Šä¸‹æ–‡
- ç»§ç»­ï¼šå¸¦ç€æ–°ä¸Šä¸‹æ–‡ç»§ç»­è°ƒç”¨æ¨¡å‹ç”Ÿæˆ

#### Q16: vLLMä¸­model.generate()è¾“å‡ºæ ¼å¼
- è¾“å‡ºæ˜¯token IDåˆ—è¡¨ï¼ˆList[int]ï¼‰ï¼Œæ¯ä¸ªæ•´æ•°ä»£è¡¨è¯è¡¨ä¸­çš„ä¸€ä¸ªtoken
- ä½¿ç”¨enumerateéå†åˆ—è¡¨è·å–ç´¢å¼•
- ä½¿ç”¨tokenizer.decode([token_id])å°†token IDè§£ç ä¸ºæ–‡å­—
- input_ids + output_idsæ˜¯ä¸¤ä¸ªæ•´æ•°åˆ—è¡¨çš„æ‹¼æ¥

---

## Phase 4: è®­ç»ƒ Trick ä¸ä¼˜åŒ–

### 4.1 FSDP åˆ†å¸ƒå¼è®­ç»ƒ

**Fully Sharded Data Parallel**ï¼šæ¨¡å‹å‚æ•°ã€æ¢¯åº¦ã€ä¼˜åŒ–å™¨çŠ¶æ€å…¨éƒ¨åˆ†ç‰‡

```yaml
actor_rollout_ref.actor.fsdp_config:
  param_offload: true      # å‚æ•°å¸è½½åˆ°CPU
  grad_offload: true       # æ¢¯åº¦å¸è½½åˆ°CPU
  optimizer_offload: true  # ä¼˜åŒ–å™¨çŠ¶æ€å¸è½½
```

**ä¼˜åŠ¿**ï¼šç”¨8å¼ GPUè®­ç»ƒ30B+æ¨¡å‹
**trade-off**ï¼šé€šä¿¡å¼€é”€å¢åŠ 

### 4.2 vLLM æ¨ç†åŠ é€Ÿ

```yaml
actor_rollout_ref.rollout:
  name: vllm
  gpu_memory_utilization: 0.6
  tensor_model_parallel_size: 1
```

**æ ¸å¿ƒæŠ€æœ¯**ï¼š
- **PagedAttention**ï¼šæ˜¾å­˜åˆ†é¡µç®¡ç†
- **Continuous Batching**ï¼šåŠ¨æ€æ‰¹å¤„ç†
- **Tensor Parallel**ï¼šå¼ é‡å¹¶è¡Œ

> ğŸ“Œ **vLLMæ·±å…¥å­¦ä¹ å¤‡æ³¨**ï¼šæ­¤éƒ¨åˆ†æ¶‰åŠè¾ƒæ·±çš„ç³»ç»Ÿå®ç°ç»†èŠ‚ï¼Œåˆå­¦è€…å¯å…ˆæŒæ¡æ ¸å¿ƒæ¦‚å¿µï¼Œåç»­æ·±å…¥å­¦ä¹ ã€‚
> - å®˜æ–¹æºç ï¼šhttps://github.com/vllm-project/vllm
> - å…¥é—¨æ•™ç¨‹ï¼šhttps://github.com/GeeeekExplorer/nano-vllm

### 4.3 è®­ç»ƒç¨³å®šæ€§æŠ€å·§

| Trick | é…ç½® | åŸç† | æ•ˆæœ |
|-------|------|------|------|
| **Gradient Checkpointing** | `enable_gradient_checkpointing=true` | ç”¨è®¡ç®—æ¢ç©ºé—´ | æ˜¾å­˜èŠ‚çœ30% |
| **KL Control** | `kl_loss_coef=0.001` | é™åˆ¶ç­–ç•¥å˜åŒ– | é˜²æ­¢è®­ç»ƒå´©æºƒ |
| **Learning Rate Warmup** | `lr_warmup_steps_ratio=0.285` | åˆæœŸå°å­¦ä¹ ç‡ | ç¨³å®šæ”¶æ•› |
| **Micro Batch** | `ppo_micro_batch_size=64` | åˆ†å‰²å¤§æ‰¹æ¬¡ | æ§åˆ¶æ˜¾å­˜å³°å€¼ |
| **Gradient Clipping** | `max_grad_norm=1.0` | é™åˆ¶æ¢¯åº¦èŒƒæ•° | é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ |

### 4.4 PPO è®­ç»ƒå‚æ•°è¡¨

```yaml
# Actor (ç­–ç•¥æ¨¡å‹)
actor_rollout_ref.actor:
  optim:
    lr: 1e-6                    # å­¦ä¹ ç‡
    lr_warmup_steps_ratio: 0.285
  use_kl_loss: true            # å¯ç”¨KLæŸå¤±
  kl_loss_coef: 0.001          # KLç³»æ•°
  kl_loss_type: low_var_kl     # KLè®¡ç®—æ–¹å¼

# Critic (å€¼å‡½æ•°æ¨¡å‹) - ä»…PPO
critic:
  optim:
    lr: 1e-5                   # é€šå¸¸æ¯”Actorå¤§
    lr_warmup_steps_ratio: 0.015
  model:
    path: ${actor_rollout_ref.model.path}

# Rollout (ç”Ÿæˆ)
actor_rollout_ref.rollout:
  n_agent: 1                    # PPO:1, GRPO:5
  temperature: 1.0              # ç”Ÿæˆæ¸©åº¦
  log_prob_micro_batch_size: 128
```

### 4.5 å­¦ä¹ ä»»åŠ¡

- [ ] ç†è§£FSDPçš„é€šä¿¡åŸç†
- [ ] å¯¹æ¯”vLLMå’Œæ™®é€šæ¨ç†çš„é€Ÿåº¦å·®å¼‚
- [ ] è°ƒæ•´KLç³»æ•°è§‚å¯Ÿè®­ç»ƒæ›²çº¿å˜åŒ–
- [ ] ç”¨ä¸åŒbatch sizeåšæ¶ˆèå®éªŒ

---

## Phase 5: æ•°æ®æµä¸åè®®

### 5.1 verl æ•°æ®æ ¼å¼

```python
# verl/DataProto ç»“æ„
DataProto:
  batch: {
    "prompts": Tensor[batch, seq_len]      # è¾“å…¥token
    "responses": Tensor[batch, seq_len]    # ç”Ÿæˆtoken
    "attention_mask": Tensor[batch, seq_len]
    "position_ids": Tensor[batch, seq_len]
  }
  non_tensor_batch: {
    "data_source": str                      # æ•°æ®é›†åç§°
    "reward_model": {
      "style": "rule"                       # å¥–åŠ±ç±»å‹
      "ground_truth": str                   # æ ‡å‡†ç­”æ¡ˆ
    }
  }
```

### 5.2 æ•°æ®å¤„ç†æµç¨‹

```python
# scripts/data_process/nq_search.py

data = {
    "data_source": "nq",
    "prompt": [{
        "role": "user",
        "content": question,
    }],
    "ability": "fact-reasoning",
    "reward_model": {
        "style": "rule",
        "ground_truth": answer
    },
    "extra_info": {
        "split": "train",
        "index": 0,
    }
}
```

### 5.3 å¥–åŠ±å‡½æ•°è®¾è®¡

```python
# verl/trainer/main_ppo.py - RewardManager

def __call__(self, data: DataProto):
    reward_tensor = torch.zeros_like(data.batch['responses'])
    
    for i in range(len(data)):
        # 1. è§£ç ç”Ÿæˆå†…å®¹
        sequences = decode(prompts + responses)
        
        # 2. æå–ç­”æ¡ˆï¼ˆæ­£åˆ™åŒ¹é…ï¼‰
        answer = extract_answer(sequences)
        
        # 3. EMåŒ¹é…è®¡ç®—åˆ†æ•°
        score = compute_em(answer, ground_truth)
        
        # 4. æ”¾åœ¨æœ€åä¸€ä¸ªtoken
        reward_tensor[i, response_length - 1] = score
    
    return reward_tensor
```

### 5.4 Ray åˆ†å¸ƒå¼æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Ray Cluster                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   Actor     â”‚  â”‚   Critic    â”‚  â”‚   Ref       â”‚  â”‚
â”‚  â”‚   Worker    â”‚  â”‚   Worker    â”‚  â”‚   Worker    â”‚  â”‚
â”‚  â”‚  (GPU 0-3)  â”‚  â”‚  (GPU 0-3)  â”‚  â”‚  (GPU 0-3)  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚           RayPPOTrainer (Master)                â”‚â”‚
â”‚  â”‚  - æ•°æ®åˆ†å‘                                      â”‚â”‚
â”‚  â”‚  - èšåˆæ¢¯åº¦                                      â”‚â”‚
â”‚  â”‚  - æ—¥å¿—è®°å½•                                      â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 5.5 å­¦ä¹ ä»»åŠ¡

- [ ] å¤ç°ä¸€ä¸ªç®€å•çš„å¥–åŠ±å‡½æ•°
- [ ] ç†è§£DataProtoçš„æ•°æ®æµ
- [ ] é…ç½®Rayé›†ç¾¤è¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒ
- [ ] å®ç°å¤šæ•°æ®æºçš„å¥–åŠ±è®¡ç®—

---

## Phase 6: å®éªŒä¸è°ƒå‚

### 6.1 æ¨èå®éªŒé¡ºåº

| é˜¶æ®µ | å®éªŒå†…å®¹ | é¢„æœŸç»“æœ |
|------|----------|----------|
| **v0.1** | 3Bæ¨¡å‹ + PPO + 1è½®æœç´¢ | æ”¶æ•›ã€å­¦ä¼šæœç´¢ |
| **v0.2** | 7Bæ¨¡å‹ + PPO + 2è½®æœç´¢ | æ›´å¥½çš„æ•ˆæœ |
| **v0.3** | GRPO + å¤šAgent | æ›´å¿«æ”¶æ•› |

### 6.2 å…³é”®è¶…å‚æ•°é€ŸæŸ¥

| å‚æ•° | é»˜è®¤å€¼ | è°ƒå‚å»ºè®® |
|------|--------|----------|
| `lr` | 1e-6 | å¢å¤§â†’æ›´å¿«æ”¶æ•›ä½†ä¸ç¨³å®š |
| `kl_coef` | 0.001 | å¢å¤§â†’ç­–ç•¥å˜åŒ–å° |
| `clip_range` | 0.2 | å‡å°â†’æ›´æ–°æ›´ä¿å®ˆ |
| `gamma` | 1.0 | å‡å°â†’æ›´å…³æ³¨çŸ­æœŸå¥–åŠ± |
| `temperature` | 1.0 | å¢å¤§â†’ç”Ÿæˆæ›´å¤šæ · |

### 6.3 ç›‘æ§æŒ‡æ ‡

| æŒ‡æ ‡ | å«ä¹‰ | å¥åº·èŒƒå›´ |
|------|------|----------|
| `reward/mean` | å¹³å‡å¥–åŠ± | é€æ¸ä¸Šå‡ |
| `kl/approx_kl` | KLæ•£åº¦ | 0.01-0.02 |
| `policy/clip_fraction` | è¢«clipçš„æ ·æœ¬æ¯”ä¾‹ | < 0.1 |
| `response/length` | ç”Ÿæˆé•¿åº¦ | é€æ¸å¢é•¿ |
| `rollout/num_searches` | æœç´¢æ¬¡æ•° | é€æ¸ç¨³å®š |

### 6.4 å­¦ä¹ ä»»åŠ¡

- [ ] å¤ç°v0.1-v0.3çš„å®éªŒ
- [ ] è°ƒæ•´è¶…å‚æ•°è®°å½•å®éªŒæ—¥å¿—
- [ ] å¯¹æ¯”PPO/GRPOçš„è®­ç»ƒæ›²çº¿
- [ ] åœ¨å…¶ä»–æ•°æ®é›†ä¸ŠéªŒè¯

---

## æ¨èå­¦ä¹ é¡ºåº

```
1. README.md
   â””â”€â”€ ç†è§£é¡¹ç›®å®šä½å’Œæ ¸å¿ƒè´¡çŒ®

2. train_ppo.sh / train_grpo.sh
   â””â”€â”€ ç†Ÿæ‚‰è®­ç»ƒé…ç½®å’Œå‚æ•°

3. verl/trainer/main_ppo.py
   â””â”€â”€ ç†è§£è®­ç»ƒä¸»æµç¨‹å’Œå¥–åŠ±è®¡ç®—

4. verl/trainer/ppo/core_algos.py
   â””â”€â”€ æŒæ¡PPO/GRPOç®—æ³•å®ç°

5. verl/workers/rollout/*.py
   â””â”€â”€ ç†è§£ç”Ÿæˆå’Œå·¥å…·è°ƒç”¨æœºåˆ¶

6. search_r1/search/retrieval.py
   â””â”€â”€ äº†è§£æ£€ç´¢ç³»ç»Ÿå®ç°
```

---

## é¢è¯•è¦ç‚¹æ€»ç»“

### æŠ€æœ¯æ·±åº¦é—®é¢˜

1. **PPOçš„Clipæœºåˆ¶åŸç†ï¼Ÿ**
   - é™åˆ¶ç­–ç•¥å˜åŒ–æ¯”ä¾‹ï¼Œé˜²æ­¢è¿‡å¤§çš„ç­–ç•¥æ›´æ–°
   - å®ç°ï¼šmin(r*A, clip(r, 1-Îµ, 1+Îµ)*A)

2. **GAEç›¸æ¯”ç®€å•TD(Î»)çš„ä¼˜åŠ¿ï¼Ÿ**
   - æ–¹å·®-åå·®æƒè¡¡æ›´çµæ´»
   - Î³æ§åˆ¶æ–¹å·®ï¼ŒÎ»æ§åˆ¶åå·®

3. **KLæ•£åº¦åœ¨RLHFä¸­çš„ä½œç”¨ï¼Ÿ**
   - é™åˆ¶æ–°ç­–ç•¥ä¸è¦åç¦»å‚è€ƒç­–ç•¥å¤ªè¿œ
   - é˜²æ­¢ç­–ç•¥å´©æºƒï¼Œä¿æŒè®­ç»ƒç¨³å®š

4. **GRPOä¸ºä»€ä¹ˆä¸éœ€è¦å€¼å‡½æ•°ï¼Ÿ**
   - ç»„å†…ç›¸å¯¹æ’åä»£æ›¿ç»å¯¹ä»·å€¼ä¼°è®¡
   - é™ä½è®¡ç®—å¼€é”€å’Œä¼°è®¡åå·®

5. **FSDPç›¸æ¯”DDPçš„ä¼˜åŠ¿ï¼Ÿ**
   - æ”¯æŒæ›´å¤§æ¨¡å‹ï¼ˆå…¨åˆ†ç‰‡ï¼‰
   - æ˜¾å­˜æ•ˆç‡æ›´é«˜

### ç³»ç»Ÿè®¾è®¡é—®é¢˜

1. **å¦‚ä½•è®¾è®¡ä¸€ä¸ªTool Calling Agentï¼Ÿ**
   - å®šä¹‰å·¥å…·è°ƒç”¨åè®®ï¼ˆAPIæ ¼å¼ï¼‰
   - è®¾è®¡æŒ‡ä»¤æ¨¡æ¿å¼•å¯¼æ¨¡å‹è°ƒç”¨
   - å®ç°è§£æå’Œè°ƒç”¨é€»è¾‘

2. **å¦‚ä½•å¤„ç†Tool Callingçš„é•¿ç¨‹ä¾èµ–ï¼Ÿ**
   - ä¿ç•™å®Œæ•´çš„å¯¹è¯å†å²
   - ä½¿ç”¨å¤šè½®æœ€å¤§é•¿åº¦é™åˆ¶

3. **Rule-based Rewardçš„å±€é™æ€§ï¼Ÿ**
   - åªèƒ½è¯„ä¼°æœ€ç»ˆç»“æœ
   - æ— æ³•å¤„ç†å¼€æ”¾å¼ä»»åŠ¡

---

## å»¶ä¼¸å­¦ä¹ èµ„æº

### è®ºæ–‡
- [Search-R1](https://arxiv.org/abs/2503.09516)
- [DeepSeek-R1](https://github.com/deepseek-ai/DeepSeek-R1)
- [PPO Paper](https://arxiv.org/abs/1707.06347)
- [GRPO Paper](https://arxiv.org/abs/2402.03300)

### å¼€æºé¡¹ç›®
- [veRL](https://github.com/volcengine/verl)
- [DeepResearcher](https://github.com/GAIR-NLP/DeepResearcher)
- [Tinker](https://github.com/thinking-machines-lab/tinker-cookbook)

### å®éªŒæ—¥å¿—
- [W&B v0.1](https://wandb.ai/peterjin/Search-R1-nq_hotpotqa_train)
- [W&B v0.2](https://wandb.ai/peterjin/Search-R1-v0.2)
- [W&B v0.3](https://wandb.ai/peterjin/Search-R1-v0.3)

---

## å­¦ä¹ æ£€æŸ¥æ¸…å•

- [x] èƒ½ä»å¤´è§£é‡ŠPPOçš„è®­ç»ƒæµç¨‹
- [x] èƒ½æ‰‹å†™GAEå’ŒPPOæŸå¤±å‡½æ•°
- [x] ç†è§£GRPOçš„ç»„ç›¸å¯¹ç­–ç•¥
- [x] äº†è§£Tool Callingçš„å®ç°åŸç†
- [ ] æŒæ¡FSDPå’ŒvLLMçš„ä½¿ç”¨
- [ ] èƒ½å¤ç°ä¸€ä¸ªå®Œæ•´çš„è®­ç»ƒå®éªŒ
- [x] èƒ½å‘é¢è¯•å®˜æ¸…æ™°è®²è§£æŠ€æœ¯ç»†èŠ‚

---

*æœ€åæ›´æ–°ï¼š2025-02-13*
