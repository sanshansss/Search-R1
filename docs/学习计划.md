# Search-R1 Agentic RL 学习计划

> 基于强化学习的推理-搜索交织LLM训练项目

---

## 项目概述

**Search-R1** 是一个强化学习框架，用于训练具备推理和搜索工具调用能力的LLM Agent。

- **核心目标**：让基础模型通过RL学会在推理过程中主动调用搜索工具
- **技术路线**：PPO/GRPO + Tool Calling + Rule-based Reward
- **对标方案**：OpenAI DeepResearch
- **开源协议**：Apache 2.0

---

## 学习目标

1. 掌握Agentic RL的核心训练范式
2. 理解PPO/GRPO算法的工程实现
3. 学会Tool Calling机制的设计与实现
4. 积累分布式训练和推理优化的经验
5. 能够向面试官清晰阐述技术细节

---

## 学习路线图

```
┌─────────────────────────────────────────────────────────────────┐
│  Phase 1: 项目架构理解 (1-2天)                                    │
├─────────────────────────────────────────────────────────────────┤
│  Phase 2: RL算法实现 (2-3天)                                     │
│     ├── PPO核心流程                                              │
│     ├── GRPO组相对策略优化                                        │
│     └── GAE优势估计                                              │
├─────────────────────────────────────────────────────────────────┤
│  Phase 3: Tool Calling机制 (2天)                                │
│     ├── 搜索API调用流程                                          │
│     ├── 多轮交互实现                                             │
│     └── 检索器选型                                              │
├─────────────────────────────────────────────────────────────────┤
│  Phase 4: 训练Trick与优化 (2-3天)                               │
│     ├── FSDP分布式训练                                          │
│     ├── vLLM推理加速                                            │
│     └── 训练稳定性调优                                           │
├─────────────────────────────────────────────────────────────────┤
│  Phase 5: 数据流与协议 (1-2天)                                  │
│     ├── verl数据格式                                            │
│     └── 奖励函数设计                                             │
├─────────────────────────────────────────────────────────────────┤
│  Phase 6: 实验复现与调参 (持续)                                  │
└─────────────────────────────────────────────────────────────────┘
```

---

## Phase 1: 项目整体架构理解

### 1.1 核心概念

| 概念 | 解释 |
|------|------|
| **Reasoning-Search Interleaved** | 推理与搜索交替进行，Agent自主决定何时搜索 |
| **Tool Calling Agent** | 能够调用外部工具（搜索API）的智能体 |
| **Rule-based RL** | 基于规则的奖励信号，无需奖励模型 |
| **Outcome Supervision** | 仅对最终结果进行监督，不监督中间过程 |

### 1.2 整体数据流

```
┌──────────────┐    ┌──────────────┐    ┌──────────────┐
│   DataLoader  │ -> │  RayPPO      │ -> │   Rollout    │
│  (Parquet)   │    │   Trainer    │    │  (vLLM)      │
└──────────────┘    └──────────────┘    └──────┬───────┘
                                               │
                    ┌──────────────────────────┘
                    v
           ┌───────────────┐    ┌──────────────┐    ┌──────────────┐
           │   LLM生成    │ -> │  Search API  │ <- │  决定是否搜索 │
           │  到特殊token │    │   调用       │    │  (<search>)  │
           └───────────────┘    └──────┬───────┘    └──────────────┘
                                       │
                                       v
                               ┌───────────────┐
                               │  Reward评估   │
                               │  (EM匹配)     │
                               └───────┬───────┘
                                       │
                                       v
                               ┌───────────────┐
                               │  PPO/GRPO更新 │
                               └───────────────┘
```

### 1.3 关键文件速览

| 路径 | 功能 |
|------|------|
| `train_ppo.sh` | PPO训练入口脚本 |
| `train_grpo.sh` | GRPO训练入口脚本 |
| `verl/trainer/main_ppo.py` | PPO训练主逻辑 |
| `verl/trainer/ppo/core_algos.py` | PPO/GRPO核心算法 |
| `verl/workers/rollout/` | Rollout生成模块 |
| `search_r1/search/retrieval.py` | 检索器实现 |
| `infer.py` | 推理测试脚本 |

### 1.4 学习任务

- [ ] 阅读README.md，理解项目定位
- [ ] 浏览train_ppo.sh，熟悉训练配置
- [ ] 查看verl目录结构，了解veRL框架
- [ ] 绘制整体架构图

---

## Phase 2: 强化学习算法实现

### 2.1 PPO算法详解

#### 核心公式

**1. GAE优势估计 (Generalized Advantage Estimation)**
```
δ_t = r_t + γ * V(s_{t+1}) - V(s_t)
A_t = δ_t + γ * λ * δ_{t+1} + (γ * λ)^2 * δ_{t+2} + ...
```

**2. PPO Clip更新**
```
L = min( r(θ) * A_t, clip(r(θ), 1-ε, 1+ε) * A_t )
```
其中 r(θ) = π_θ(a|s) / π_θ_old(a|s)

**3. KL散度惩罚**
```
KL = log(π_new) - log(π_old)
Loss = L - β * KL
```

#### 代码实现位置

```python
# verl/trainer/ppo/core_algos.py

def compute_gae_advantage_return(token_level_rewards, values, eos_mask, gamma, lam):
    """GAE优势计算"""
    # δ_t = r_t + γ * V_{t+1} - V_t
    # A_t = Σ (γλ)^l * δ_l

def compute_policy_loss(old_log_prob, log_prob, advantages, eos_mask, cliprange):
    """PPO策略损失"""
    # L = min(r * A, clip(r, 1-ε, 1+ε) * A)

def compute_value_loss(vpreds, returns, values, eos_mask, cliprange_value):
    """值函数损失"""
    # L = 0.5 * max((v - target)^2, (clip(v) - target)^2)
```

### 2.2 GRPO算法详解

#### 核心思想

GRPO (Group Relative Policy Optimization) 不需要值函数，通过组内相对排名计算优势：

```python
# 对每个问题生成G个答案
scores = [score_1, score_2, ..., score_G]

# 计算组内均值和标准差
mean = Σscores / G
std = sqrt(Σ(scores - mean)^2 / G)

# 相对优势
A_i = (score_i - mean) / (std + ε)
```

#### 代码实现

```python
def compute_grpo_outcome_advantage(token_level_rewards, eos_mask, index, epsilon=1e-6):
    """GRPO优势计算"""
    # 1. 按问题分组
    # 2. 计算组内均值和标准差
    # 3. 标准化得分作为优势
```

### 2.3 PPO vs GRPO 对比

| 维度 | PPO | GRPO |
|------|-----|------|
| **值函数** | 需要Critic模型 | 不需要 |
| **样本效率** | 较低（每个问题1个答案） | 较高（每个问题N个答案） |
| **训练稳定性** | 较好（Clip机制） | 一般 |
| **计算开销** | 大（双模型+值函数） | 小（单模型） |
| **适用场景** | 复杂任务、长期规划 | 简单任务、快速验证 |

### 2.4 关键参数含义

```yaml
# PPO配置示例
algorithm:
  adv_estimator: gae  # 或 grpo
  gamma: 1.0          # 折扣因子
  lam: 0.95           # GAE lambda

actor_rollout_ref.actor:
  ppo_micro_batch_size: 64    # 每个micro batch的大小
  ppo_mini_batch_size: 256    # 每个mini batch的大小
  cliprange: 0.2              # PPO clip范围
  kl_loss_coef: 0.001         # KL系数
```

### 2.5 学习任务

- [ ] 精读 `core_algos.py`，手写每个核心函数
- [ ] 对比PPO和GRPO的优缺点
- [ ] 理解GAE中γ和λ的作用
- [ ] 推导PPO Gradient公式

---

## Phase 3: Tool Calling 机制

### 3.1 Agent决策流程

```
User: "Who is the CEO of Tesla?"

1. LLM生成: "Let me think... I need to search for this information <search>"
2. 检测到<search> token
3. 暂停生成，提取查询词: "Tesla CEO"
4. 调用搜索API: POST /retrieve?query="Tesla CEO"
5. 获得检索结果:
   {
     "docs": [
       {"title": "Elon Musk", "contents": "Elon Musk is the CEO..."},
       {"title": "Tesla Leadership", "contents": "Tesla was founded..."}
     ]
   }
6. 将结果插入对话:
   [{"role": "user", "content": "..."}, 
    {"role": "assistant", "content": "Let me search..."},
    {"role": "tool", "content": "[docs...]"}]
7. LLM继续生成: "Based on the search results, Elon Musk is the CEO..."
8. 最终答案验证
```

### 3.2 多轮搜索配置

```yaml
# 最大2轮搜索
max_turns: 2

# 每次检索top-3结果
retriever:
  topk: 3
  url: "http://127.0.0.1:8000/retrieve"
```

### 3.3 检索器类型

| 类型 | 实现 | 优点 | 缺点 |
|------|------|------|------|
| **BM25** | Pyserini | 轻量、无需训练 | 语义理解弱 |
| **Dense (E5)** | Faiss + Encoder | 语义匹配强 | 需要索引构建 |
| **Dense (BGE)** | Faiss + Encoder | 中文效果好 | 资源消耗大 |
| **在线搜索** | Google/Bing API | 实时数据 | 成本高、有延迟 |

### 3.4 检索器代码实现

```python
# search_r1/search/retrieval.py

class DenseRetriever:
    def __init__(self, config):
        self.index = faiss.read_index(config.index_path)
        self.encoder = Encoder(model_path=config.retrieval_model_path)
    
    def _search(self, query, num=3):
        # 1. query编码
        query_emb = self.encoder.encode(query)
        # 2. Faiss检索
        scores, idxs = self.index.search(query_emb, k=num)
        # 3. 返回文档
        results = load_docs(self.corpus, idxs)
        return results
```

### 3.5 学习任务

- [ ] 复现一个简单的BM25检索器
- [ ] 理解E5/BGE的编码和索引原理
- [ ] 设计一个Agent Tool Calling的提示模板
- [ ] 测试多轮搜索的效果

---

## Phase 4: 训练 Trick 与优化

### 4.1 FSDP 分布式训练

**Fully Sharded Data Parallel**：模型参数、梯度、优化器状态全部分片

```yaml
actor_rollout_ref.actor.fsdp_config:
  param_offload: true      # 参数卸载到CPU
  grad_offload: true       # 梯度卸载到CPU
  optimizer_offload: true  # 优化器状态卸载
```

**优势**：用8张GPU训练30B+模型
**trade-off**：通信开销增加

### 4.2 vLLM 推理加速

```yaml
actor_rollout_ref.rollout:
  name: vllm
  gpu_memory_utilization: 0.6
  tensor_model_parallel_size: 1
```

**核心技术**：
- **PagedAttention**：显存分页管理
- **Continuous Batching**：动态批处理
- **Tensor Parallel**：张量并行

### 4.3 训练稳定性技巧

| Trick | 配置 | 原理 | 效果 |
|-------|------|------|------|
| **Gradient Checkpointing** | `enable_gradient_checkpointing=true` | 用计算换空间 | 显存节省30% |
| **KL Control** | `kl_loss_coef=0.001` | 限制策略变化 | 防止训练崩溃 |
| **Learning Rate Warmup** | `lr_warmup_steps_ratio=0.285` | 初期小学习率 | 稳定收敛 |
| **Micro Batch** | `ppo_micro_batch_size=64` | 分割大批次 | 控制显存峰值 |
| **Gradient Clipping** | `max_grad_norm=1.0` | 限制梯度范数 | 防止梯度爆炸 |

### 4.4 PPO 训练参数表

```yaml
# Actor (策略模型)
actor_rollout_ref.actor:
  optim:
    lr: 1e-6                    # 学习率
    lr_warmup_steps_ratio: 0.285
  use_kl_loss: true            # 启用KL损失
  kl_loss_coef: 0.001          # KL系数
  kl_loss_type: low_var_kl     # KL计算方式

# Critic (值函数模型) - 仅PPO
critic:
  optim:
    lr: 1e-5                   # 通常比Actor大
    lr_warmup_steps_ratio: 0.015
  model:
    path: ${actor_rollout_ref.model.path}

# Rollout (生成)
actor_rollout_ref.rollout:
  n_agent: 1                    # PPO:1, GRPO:5
  temperature: 1.0              # 生成温度
  log_prob_micro_batch_size: 128
```

### 4.5 学习任务

- [ ] 理解FSDP的通信原理
- [ ] 对比vLLM和普通推理的速度差异
- [ ] 调整KL系数观察训练曲线变化
- [ ] 用不同batch size做消融实验

---

## Phase 5: 数据流与协议

### 5.1 verl 数据格式

```python
# verl/DataProto 结构
DataProto:
  batch: {
    "prompts": Tensor[batch, seq_len]      # 输入token
    "responses": Tensor[batch, seq_len]    # 生成token
    "attention_mask": Tensor[batch, seq_len]
    "position_ids": Tensor[batch, seq_len]
  }
  non_tensor_batch: {
    "data_source": str                      # 数据集名称
    "reward_model": {
      "style": "rule"                       # 奖励类型
      "ground_truth": str                   # 标准答案
    }
  }
```

### 5.2 数据处理流程

```python
# scripts/data_process/nq_search.py

data = {
    "data_source": "nq",
    "prompt": [{
        "role": "user",
        "content": question,
    }],
    "ability": "fact-reasoning",
    "reward_model": {
        "style": "rule",
        "ground_truth": answer
    },
    "extra_info": {
        "split": "train",
        "index": 0,
    }
}
```

### 5.3 奖励函数设计

```python
# verl/trainer/main_ppo.py - RewardManager

def __call__(self, data: DataProto):
    reward_tensor = torch.zeros_like(data.batch['responses'])
    
    for i in range(len(data)):
        # 1. 解码生成内容
        sequences = decode(prompts + responses)
        
        # 2. 提取答案（正则匹配）
        answer = extract_answer(sequences)
        
        # 3. EM匹配计算分数
        score = compute_em(answer, ground_truth)
        
        # 4. 放在最后一个token
        reward_tensor[i, response_length - 1] = score
    
    return reward_tensor
```

### 5.4 Ray 分布式架构

```
┌─────────────────────────────────────────────────────┐
│                   Ray Cluster                        │
├─────────────────────────────────────────────────────┤
│                                                     │
│  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐ │
│  │   Actor     │  │   Critic    │  │   Ref       │ │
│  │   Worker    │  │   Worker    │  │   Worker    │ │
│  │  (GPU 0-3)  │  │  (GPU 0-3)  │  │  (GPU 0-3)  │ │
│  └─────────────┘  └─────────────┘  └─────────────┘ │
│                                                     │
│  ┌─────────────────────────────────────────────────┐│
│  │           RayPPOTrainer (Master)                ││
│  │  - 数据分发                                      ││
│  │  - 聚合梯度                                      ││
│  │  - 日志记录                                      ││
│  └─────────────────────────────────────────────────┘│
│                                                     │
└─────────────────────────────────────────────────────┘
```

### 5.5 学习任务

- [ ] 复现一个简单的奖励函数
- [ ] 理解DataProto的数据流
- [ ] 配置Ray集群进行分布式训练
- [ ] 实现多数据源的奖励计算

---

## Phase 6: 实验与调参

### 6.1 推荐实验顺序

| 阶段 | 实验内容 | 预期结果 |
|------|----------|----------|
| **v0.1** | 3B模型 + PPO + 1轮搜索 | 收敛、学会搜索 |
| **v0.2** | 7B模型 + PPO + 2轮搜索 | 更好的效果 |
| **v0.3** | GRPO + 多Agent | 更快收敛 |

### 6.2 关键超参数速查

| 参数 | 默认值 | 调参建议 |
|------|--------|----------|
| `lr` | 1e-6 | 增大→更快收敛但不稳定 |
| `kl_coef` | 0.001 | 增大→策略变化小 |
| `clip_range` | 0.2 | 减小→更新更保守 |
| `gamma` | 1.0 | 减小→更关注短期奖励 |
| `temperature` | 1.0 | 增大→生成更多样 |

### 6.3 监控指标

| 指标 | 含义 | 健康范围 |
|------|------|----------|
| `reward/mean` | 平均奖励 | 逐渐上升 |
| `kl/approx_kl` | KL散度 | 0.01-0.02 |
| `policy/clip_fraction` | 被clip的样本比例 | < 0.1 |
| `response/length` | 生成长度 | 逐渐增长 |
| `rollout/num_searches` | 搜索次数 | 逐渐稳定 |

### 6.4 学习任务

- [ ] 复现v0.1-v0.3的实验
- [ ] 调整超参数记录实验日志
- [ ] 对比PPO/GRPO的训练曲线
- [ ] 在其他数据集上验证

---

## 推荐学习顺序

```
1. README.md
   └── 理解项目定位和核心贡献

2. train_ppo.sh / train_grpo.sh
   └── 熟悉训练配置和参数

3. verl/trainer/main_ppo.py
   └── 理解训练主流程和奖励计算

4. verl/trainer/ppo/core_algos.py
   └── 掌握PPO/GRPO算法实现

5. verl/workers/rollout/*.py
   └── 理解生成和工具调用机制

6. search_r1/search/retrieval.py
   └── 了解检索系统实现
```

---

## 面试要点总结

### 技术深度问题

1. **PPO的Clip机制原理？**
   - 限制策略变化比例，防止过大的策略更新
   - 实现：min(r*A, clip(r, 1-ε, 1+ε)*A)

2. **GAE相比简单TD(λ)的优势？**
   - 方差-偏差权衡更灵活
   - γ控制方差，λ控制偏差

3. **KL散度在RLHF中的作用？**
   - 限制新策略不要偏离参考策略太远
   - 防止策略崩溃，保持训练稳定

4. **GRPO为什么不需要值函数？**
   - 组内相对排名代替绝对价值估计
   - 降低计算开销和估计偏差

5. **FSDP相比DDP的优势？**
   - 支持更大模型（全分片）
   - 显存效率更高

### 系统设计问题

1. **如何设计一个Tool Calling Agent？**
   - 定义工具调用协议（API格式）
   - 设计指令模板引导模型调用
   - 实现解析和调用逻辑

2. **如何处理Tool Calling的长程依赖？**
   - 保留完整的对话历史
   - 使用多轮最大长度限制

3. **Rule-based Reward的局限性？**
   - 只能评估最终结果
   - 无法处理开放式任务

---

## 延伸学习资源

### 论文
- [Search-R1](https://arxiv.org/abs/2503.09516)
- [DeepSeek-R1](https://github.com/deepseek-ai/DeepSeek-R1)
- [PPO Paper](https://arxiv.org/abs/1707.06347)
- [GRPO Paper](https://arxiv.org/abs/2402.03300)

### 开源项目
- [veRL](https://github.com/volcengine/verl)
- [DeepResearcher](https://github.com/GAIR-NLP/DeepResearcher)
- [Tinker](https://github.com/thinking-machines-lab/tinker-cookbook)

### 实验日志
- [W&B v0.1](https://wandb.ai/peterjin/Search-R1-nq_hotpotqa_train)
- [W&B v0.2](https://wandb.ai/peterjin/Search-R1-v0.2)
- [W&B v0.3](https://wandb.ai/peterjin/Search-R1-v0.3)

---

## 学习检查清单

- [ ] 能从头解释PPO的训练流程
- [ ] 能手写GAE和PPO损失函数
- [ ] 理解GRPO的组相对策略
- [ ] 了解Tool Calling的实现原理
- [ ] 掌握FSDP和vLLM的使用
- [ ] 能复现一个完整的训练实验
- [ ] 能向面试官清晰讲解技术细节

---

*最后更新：2025-02*
